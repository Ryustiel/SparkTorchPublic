"""
Combined Spark and Torch workflow for training using the TorchDistributor.
"""

import kagglehub, pathlib, functools, pandas as pd, numpy as np, pyarrow.parquet, pyarrow.dataset
import pyspark.sql, pyspark.ml, pyspark.sql.functions as functions
import torch, pyspark.ml.torch.distributor, torch.distributed, torch.nn.parallel as torch_parallel
# NOTE : Full import paths are kept for "remembering" where stuff comes from, to help me learn

from typing import Dict, List, Callable
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType, ArrayType

from components import SequencerEstimator, SimpleScalerEstimator, CustomVectorAssembler, SimpleScalerModel

# ============================ TRAIN DEPENDENCIES ======================

# Because the TorchDistributor can't handle locally imported modules within the training function,
# the necessary components (dataset, model, transform function) are defined here

def spark_to_model_input(
    batch_df: pd.DataFrame, 
    sequence_features: list, 
    static_features: list, 
    target: str
) -> Dict[str, torch.Tensor]:
    """
    This function takes a batch of data generated by the spark preprocessing step,
    and does the necessary stacking and transposing to create the final tensors.
    """
    sequence_columns = [f"{col}_sequence" for col in sequence_features]
    mask_columns = [f"{col}_mask" for col in sequence_features]

    # Build a python list (nfeatures) of 2D arrays (n_samples, sequence_length)
    sequence_arrays_2d = [np.stack(batch_df[col].values) for col in sequence_columns]  # type: ignore
    mask_arrays_2d = [np.stack(batch_df[col].values) for col in mask_columns]  # type: ignore
    
   # Stack to (num_samples, sequence length, feature)
    sequences = np.stack(sequence_arrays_2d, axis=1).astype(np.float32).transpose((0, 2, 1))
    masks = np.stack(mask_arrays_2d, axis=1).astype(np.float32).transpose((0, 2, 1))
    statics = batch_df[static_features].values.astype(np.float32)
    targets = batch_df[target].values.astype(np.float32).reshape(-1, 1)

    return {
        'sequences': torch.tensor(sequences),
        'masks': torch.tensor(masks),
        'statics': torch.tensor(statics),
        'targets': torch.tensor(targets),
    }
    
class ParquetIterableDataset(torch.utils.data.IterableDataset):
    """
    An iterable dataset that reads a Parquet file/dataset in batches using pyarrow,
    applies a transformation, and yields model-ready tensor dictionaries.
    This is memory-efficient as it streams data from disk.
    """
    def __init__(self, 
        file_path: str, 
        transform_func: Callable[[pd.DataFrame], Dict[str, torch.Tensor]],
        batch_size: int,
        cur_shard: int = 0,
        num_shards: int = 1,
    ):
        super().__init__()
        self.file_path = file_path
        self.batch_size = batch_size
        self.transform_func = transform_func
        
        # Plan for distributed training (if needed)
        dataset = pyarrow.parquet.ParquetDataset(self.file_path)
        self.shard_fragments: List[pyarrow.dataset.ParquetFileFragment] = [
            fragment for i, fragment in enumerate(dataset.fragments)
            if i % num_shards == cur_shard  # Ensure each shard gets a subset of fragments (round robin kinda way)
        ]
        
        self.num_batches = sum(
            (fragment.count_rows() + batch_size - 1) // batch_size  # Ceiling division
            for fragment in self.shard_fragments
        )
        
    def __iter__(self):
        """
        Target a specific row group in parquet file based on worker info.
        """
        if len(self.shard_fragments) == 0:
            return  # No data for this shard
        
        for fragment in self.shard_fragments:
            scanner = fragment.scanner(batch_size=self.batch_size)
            # TODO : Shuffle batches somehow ?
            for batch in scanner.to_batches():
                df_batch = batch.to_pandas()
                yield self.transform_func(df_batch)

# LSTM

class LSTMRegressor(torch.nn.Module):
    
    def __init__(self, seq_input_size, still_input_size, still_hidden_size, seq_hidden_size):
        super(LSTMRegressor, self).__init__()
        self.lstm = torch.nn.LSTM(seq_input_size, seq_hidden_size, batch_first=True)
        self.still_fc = torch.nn.Linear(still_input_size, still_hidden_size)
        self.merge_fc = torch.nn.Linear(seq_hidden_size + still_hidden_size, 1)
        self.still_hidden_size = still_hidden_size
        self.seq_hidden_size = seq_hidden_size

    def forward(self, seq_x: torch.Tensor, seq_mask: torch.Tensor, still_x: torch.Tensor):
        
        batch_size = seq_x.size(0)
        device = seq_x.device
        
        processed_still = torch.nn.functional.relu(self.still_fc(still_x))  # Process ALL of the batch

        # mask.sum(dim=1) yields Batch x Features, reduce to only shape (Batch,) for pack padded sequence
        lengths = seq_mask.sum(dim=1).select(1, 0).cpu()
        
        # Only process through the LSTM those with a sequence length > 0
        has_sequence_mask = lengths > 0
        combined_features = torch.zeros(batch_size, self.still_hidden_size + self.seq_hidden_size, device=device)
        combined_features[:, self.seq_hidden_size:] = processed_still  # Fill in the static part for all rows

        lstm_out_features = torch.zeros(batch_size, self.seq_hidden_size, device=device)
        
        if has_sequence_mask.any():  # Run LSTM
            seq_x_with_data = seq_x[has_sequence_mask]
            lengths_with_data = lengths[has_sequence_mask]

            packed_input = torch.nn.utils.rnn.pack_padded_sequence(
                seq_x_with_data, 
                lengths_with_data.cpu(), 
                batch_first=True, 
                enforce_sorted=False
            )

            _, (h_n, c_n) = self.lstm(packed_input)
            last_hidden_state = h_n[-1] # (sub_batch_size, hidden_size)
            
            # Replace the zero rows with the LSTM output for those with sequences
            lstm_out_features[has_sequence_mask] = last_hidden_state
            
        if self.training and not has_sequence_mask.any():
            # Add a zero-valued term that depends on the LSTM parameters.
            # This forces autograd to "see" the parameters without changing the output.
            dummy_sum = sum(p.sum() for p in self.lstm.parameters())
            lstm_out_features = lstm_out_features + 0.0 * dummy_sum

        combined_features = torch.cat((lstm_out_features, processed_still), dim=1)
        out = self.merge_fc(combined_features)
        return out

# ============================ SPARK SETUP =============================

spark = (
    pyspark.sql.SparkSession
    .builder
    .appName("ElectricityConsumptionLSTM")
    .config("spark.sql.shuffle.partitions", "2")  # To test multi parquet files.
    .getOrCreate()
)

spark.sparkContext.setLogLevel("WARN")

# Tells spark to interpret ../07 as year 2007, specific to current dataset
spark.conf.set("spark.sql.legacy.timeParserPolicy", "LEGACY")

target_folder = pathlib.Path("./data").absolute()  # Safer for distributed environments
target_folder.mkdir(exist_ok=True)

# file_path = kagglehub.dataset_download(
#     handle="samxsam/household-energy-consumption",
#     path="household_energy_consumption.csv",
# )
file_path = str(target_folder / "household_energy_consumption.csv")

df = spark.read.csv(
    file_path,
    header=True,
    schema = StructType([
        StructField("Household_ID", StringType(), nullable=False),
        StructField("Date", DateType(), nullable=False),
        StructField("Energy_Consumption_kWh", DoubleType(), nullable=False),
        StructField("Household_Size", IntegerType(), nullable=False),
        StructField("Avg_Temperature_C", DoubleType(), nullable=False),
        StructField("Has_AC", StringType(), nullable=False), 
        StructField("Peak_Hours_Usage_kWh", DoubleType(), nullable=False),
    ]),
).withColumn("Has_AC", functions.when(functions.col("Has_AC") == "Yes", True).otherwise(False))

# ============================= PIPELINE =============================

# Layers

scaler = SimpleScalerEstimator(
    inputCols=["Energy_Consumption_kWh", "Household_Size", "Avg_Temperature_C"],
)
sequencer = SequencerEstimator(  # Create a _sequence and a _mask vector column for each input feature, scale them
    partition_col="Household_ID",
    order_col="Date",
    sequence_features_cols=["Avg_Temperature_C", "Peak_Hours_Usage_kWh", "Energy_Consumption_kWh"],
    sequence_length=5,
)
aggregator = CustomVectorAssembler(
    inputCols=["Household_Size_scaled", "Avg_Temperature_C_scaled", "Has_AC"],
    outputCol="static_features",
)

pipeline = pyspark.ml.Pipeline(stages=[scaler, sequencer])

pipeline_model = pipeline.fit(df)
df = pipeline_model.transform(df)

households = df.select("Household_ID").distinct()
train_id, validate_id = households.randomSplit([0.9, 0.1], seed=42)
train_df, validate_df = df.join(train_id, on="Household_ID", how="semi"), df.join(validate_id, on="Household_ID", how="semi")

train_df.orderBy(functions.rand()).write.mode("overwrite").parquet(str(target_folder / "energy_data_train"))
validate_df.write.mode("overwrite").parquet(str(target_folder / "energy_data_validate"))

# ============================= TRAIN =============================

spark_preprocess_transform = functools.partial(
        spark_to_model_input,
        sequence_features=["Avg_Temperature_C", "Peak_Hours_Usage_kWh", "Energy_Consumption_kWh"],
        static_features=["Household_Size_scaled", "Avg_Temperature_C_scaled", "Has_AC"],
        target="Energy_Consumption_kWh_scaled"
    )

def training_loop():
    
    torch.distributed.init_process_group(backend="gloo")  # 'gloo' for CPU, 'nccl' for GPU
    
    rank = torch.distributed.get_rank()
    world_size = torch.distributed.get_world_size()
    device = torch.device(f"cpu")  # ("cuda" for GPU)
    
    train_dataset = ParquetIterableDataset(
        file_path=str(target_folder / "energy_data_train"),
        batch_size=500,
        cur_shard=rank,  # Will target a specific subset of the parquet files (if more processes are used than dataset files created then they won't have any workload because of the modulo condition)
        num_shards=world_size,
        transform_func=spark_preprocess_transform
    )
    
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=None)

    model = LSTMRegressor(seq_input_size=3, still_input_size=3, still_hidden_size=64, seq_hidden_size=64).to(device)
    model = torch_parallel.DistributedDataParallel(model, find_unused_parameters=True)  # Because of the masks some parameters are not used in gradient computation
    
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = torch.nn.MSELoss()
    num_epochs = 6
    
    for epoch in range(num_epochs):
        
        model.train()  # Put model in training mode
        total_weighted_loss = 0
        total_samples = 0
        
        # NOTE : Can compute a global step size (from the minimum number of batches across all shards) to have the same number of operations
        
        # Allow ranks that finish early to join without breaking collectives
        with model.join():
            for data in train_loader:
                optimizer.zero_grad()

                predictions = model(
                    seq_x=data["sequences"].to(device), 
                    seq_mask=data["masks"].to(device), 
                    still_x=data["statics"].to(device)
                )

                loss = criterion(predictions, data["targets"].to(device))
                loss.backward()  # DDP automatically averages gradients across all processes
                optimizer.step()
                
                bsz = data["targets"].size(0)  # Batch size (may be smaller for the last batch)
                total_weighted_loss += loss.item() * bsz  # Not an average
                total_samples += bsz
            
        # TODO MAYBE : Early stopping, gradient clipping, lr scheduler, etc.
        
        loss_sum_tensor = torch.tensor([total_weighted_loss], device=device)
        sample_sum_tensor = torch.tensor([total_samples], device=device, dtype=torch.long)

        torch.distributed.all_reduce(loss_sum_tensor, op=torch.distributed.ReduceOp.SUM)
        torch.distributed.all_reduce(sample_sum_tensor, op=torch.distributed.ReduceOp.SUM)

        global_samples = int(sample_sum_tensor.item())
        global_avg_loss = loss_sum_tensor.item() / max(global_samples, 1)

        # Only print from the rank 0 process to avoid cluttered logs.
        if rank == 0:
            print(f"Epoch {epoch+1}/{num_epochs}, Global Avg Train Loss: {global_avg_loss:.4f}")
            
    if rank == 0:
        # When using DDP, the original model is stored in the `.module` attribute.
        torch.save(model.module.state_dict(), str(target_folder / "lstm_model.pth"))
    
    torch.distributed.destroy_process_group()  # This is a cleanup

distributor = pyspark.ml.torch.distributor.TorchDistributor(num_processes=2, local_mode=True, use_gpu=False)
distributor.run(training_loop)  # Train and Save

# ============================ EVAL =============================

print("\nEvaluating performances...")
validation_dataset = ParquetIterableDataset(
        file_path=str(target_folder / "energy_data_validate"),
        batch_size=500,
        transform_func=spark_preprocess_transform
    )
val_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=None)

model = LSTMRegressor(seq_input_size=3, still_input_size=3, still_hidden_size=64, seq_hidden_size=64)
model.load_state_dict(torch.load(target_folder / "lstm_model.pth"))
criterion = torch.nn.MSELoss()

scaler_model: SimpleScalerModel = pipeline_model.stages[0]  # Retrieve the scaler from the pipeline  # type: ignore
y_scaling_parameters = scaler_model.get_params("Energy_Consumption_kWh")
print("Scaling parameters to retrieve the original value", y_scaling_parameters)

model.eval() # Put the model in evaluation mode (disables dropout, etc.)
total_val_loss = 0
with torch.no_grad():
    for data in val_loader:
        predictions = model(seq_x=data["sequences"], seq_mask=data["masks"], still_x=data["statics"])
        loss = criterion(predictions, data["targets"])
        total_val_loss += loss.item()

avg_val_loss = total_val_loss / validation_dataset.num_batches

print(f"Val Loss: {avg_val_loss:.4f}")
